\chapter{Erasure codes tester}

\section{Summary}

The erasure codes tester is a program written in Java that exposes a filesystem to the user.
It is particular in the way it stores files data: they are saved in a key-value store after being processed by an erasure coding algorithm.
Each of the three components (filesystem interface, key-value store, erasure code) can be replaced to test different implementations against each other.

\begin{figure}
	\centering
	\input{architecture-figure.tex}
	\caption{Graphical representation of the components of the system}
	\label{fig:architecture}
\end{figure}

\section{Software architecture}
\label{subsec:architecture}

This section presents the architecture of the system.
It is summarized in \autoref{fig:architecture}.

The Java program exposes a filesystem by using the \ac{fuse} interface.
When the user runs the program, the filesystem is mounted under a directory of its choice.
The user can then use this directory like it would with any other one.
The tester will intercept system calls by making use of the \textit{fuse-jna} \autocite{fuse-jna} Java library.
Read and write calls will be passed through to the encoder/decoder layer.

The encoder/decoder layer will handle the complicated task of correctly chunking the data.
Its role includes the handling of aligning read and write operations to the correct boundaries.
Once the data is chunked in blocks of the correct size, it is passed to the erasure coding algorithm, which will add redundancy blocks.
The data and parity blocks are then passed to the storage backend.

The storage backend is the component that interfaces the block storage primitives of our program to operations on a key-value store.

\subsection{Available implementations}

As stated in \autoref{subsec:architecture}, each component of our tester can have several implementations.
The frontend has one available implementation: a \ac{fuse} filesystem interface backed by \textit{fuse-jna} \autocite{fuse-jna}.
The encoder/decoder layer is not modular because it is sufficient to have one correct way of handling that operation.
There are three erasure coding algorithms bundled with the program.
They come from \autocite{XorbasVLDB}, and were adapted to free them from dependencies on any \textit{Hadoop} component.
A fourth algorithm is provided, in the name of the \textit{Null} encoder.
As its name suggests, its \textit{modus operandi} consists in simply forwarding data blocks without any added redundancy.
Our program is compatible with one distributed key-value store: \textit{Redis}.
The compatibility is possible through two libraries: \textit{Redisson} and \textit{Jedis}.
The implementation using \textit{Redisson} is simpler in terms of coding, but is slower.
It is therefore recommended to use the \textit{Jedis} implementation.
For testing purposes, a storage backend using a single Java Map is also available.

\subsection{Blocks storage}

For performance reasons\footnote{This additional component provides a considerable speed-up comprised between $100\times$ and $1000\times$}, there is an additional layer before the storage of blocks in the key-value store.
As the size of each block is 1 byte, performing an operation on the key-value store over the network for each block is very costly.
For this reason, the role of this intermediate layer is to aggregate multiple operations at once.
Instead of storing one block per key in the key-value store, each key stores an aggregation of multiple blocks.
The guarantees offered by the erasure coding process are still kept because one aggregation only stores blocks that belong to the same stripe position.

A \ac{lru} cache optimizes the retrieval of multiple individuals blocks.
This way, when reading a file sequentially, each blocks aggregation is only retrieved once from the key-value store.

\section{Surrounding components}

Running the tester requires multiple independent services.
They need to be launched simultaneously and need to be bootstrapped to work together.
On top of these services, we want to perform measurements on the performance of different erasure codes.
The solution we adopted is the containerization of each service.
The tester along with supporting Python scripts are bundled together in a Docker image.
When an experimenter wants to start the benchmarks with, say, a Redis cluster as storage backend, he only needs to ask Docker Compose to scale the number of running containers.
The supporting Python scripts will configure the Redis cluster and then run the benchmarks and collect the results.

Thanks to the use of Docker Swarm, the setup to run the benchmarks on a local machine or on a cluster of machines is similar.
The system can use any reasonable number of Redis servers.
