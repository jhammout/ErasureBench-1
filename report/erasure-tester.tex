\section{Erasure codes tester}

\subsection{Summary}

The erasure codes tester is a program written in Java that exposes a filesystem to the user.
It is particular in the way it stores files data{stores files data? Rewrite.}: they are saved in a key-value store after being processed by an erasure coding algorithm.
Each of the three components (filesystem interface, key-value store, erasure code) can be replaced to test different implementations against each other.
While the Java program can be used standalone, it is designed to run in a Docker container shared with associated Python scripts.

\subsection{Erasure codes}

The motivation to create this piece of software comes from the desire to check claims expressed in \citetitle{XorbasVLDB} \autocite{XorbasVLDB}.
The authors have developed an erasure coding algorithm in the family of \acfp{lrc}.
The reference implementation of this algorithm is open-source \autocite{xorbas-github}.
We capitalized on this fact to include it, along with other algorithms provided in their implementation, within our own tester.
The following erasure coding algorithms are available in our tester:
\begin{description}[\IEEEsetlabelwidth{XOR}]
\item[XOR] A simple XOR algorithm with 2 data blocks and 1 parity block.
\item[\acs{rs}] The classical \ac{rs} code \autocite{reed-solomon}.
\item[\acs{lrc}] The primary target of our evaluation. This code is based on \ac{rs}, but adds a layer enabling faster repair of unavailable blocks, at the expense of additional storage overhead \autocite{XorbasVLDB}.
\end{description}

Each of these algorithms take parameters that define the number of blocks devolved to each aspect of erasure-coding.
XOR takes a single parameter, which defines how many data blocks are XORed together to form the parity block.
In our case, we always use 2 data blocks for 1 parity block.
XOR can only recover from the loss of a single block.

The parameters of \iac{rs} code are written as $(k,n-k)$ where $k$ is the number of data blocks and $n-k$ is the number of parity blocks.
\acp{lrc} admit an extra parameter called $r$ (producing a $(k,n-k,r)$ triplet) that defines the block locality.
A block locality of $r$ means that any block is a function of at most $r$ other blocks \autocite{XorbasVLDB}.
In both cases, any $k$ blocks are sufficient to retrieve the data of the complete stripe.
In the \nameref{sec:evaluation}, we will use $(10,4)$ and $(10,6,5)$ as the default parameters for \ac{rs} and \ac{lrc}, respectively.

In addition to these three algorithm, our program also provides the possibility to disable erasure coding for testing purposes.
In that case, there is still one parameter to set, which is the number of data blocks.
We use 10 to be able to compare results between the latter three algorithms.

\subsection{Software architecture}
\label{subsec:architecture}

\begin{figure*}
	\centering
	\input{architecture-figure.tex}
	\caption{Graphical representation of the components of the system}
	\label{fig:architecture}
\end{figure*}

This section presents the architecture of the system.
It is summarized in \autoref{fig:architecture}.

The Java program exposes a filesystem by using the \ac{fuse} interface.
When the user runs the program, the filesystem is mounted under a directory of his choice.
The user can then use this directory like it would with any other one.
The tester will intercept system calls by making use of the \textit{fuse-jna} \autocite{fuse-jna} Java library.
Read and write calls will be passed through to the encoder/decoder layer.

The encoder/decoder layer will handle the complicated task of correctly chunking the data.
Its role includes the handling of aligning read and write operations to the correct boundaries.
Once the data is chunked in blocks of the correct size, it is passed to the erasure coding algorithm, which will add redundancy blocks.
The data and parity blocks are then passed to the storage backend.

The storage backend is the component that interfaces the block storage primitives of our program to operations on a key-value store.

\subsubsection{Available implementations}

As stated in \autoref{subsec:architecture}, each component of our tester can have several implementations.
The frontend has one available implementation: a \ac{fuse} filesystem interface backed by \textit{fuse-jna} \autocite{fuse-jna}.
The encoder/decoder layer is not modular because it is sufficient to have one correct way of handling that operation.
There are three erasure coding algorithms bundled with the program.
They come from \autocite{XorbasVLDB}, and were adapted to free them from dependencies on any \textit{Hadoop} component.
A fourth algorithm is provided, in the name of the \textit{Null} encoder.
As its name suggests, its \textit{modus operandi} consists in simply forwarding data blocks without any added redundancy.
Our program is compatible with the \textit{Redis} distributed key-value store through the \textit{Jedis} library.
For testing purposes, a storage backend using an in-memory hash map is also available.

\subsubsection{Blocks storage}

\begin{figure}[H]
    \centering
    \input{blocks-figure.tex}
    \caption{Representation of the process of splitting the data in blocks, applying erasure coding to them, and finally store them in a Redis cluster.}
    \label{fig:blocks}
\end{figure}

The storage backend is designed to operate on 1 byte blocks.
Storing each block individually in the key-value store is a very slow operation.
The metadata accompanying each request to the key-value store server is much bigger than the data itself, leading to unworkable overheads.
We therefore decided to add an intermediate layer between the encoder/decoder and the storage backend.

This layer is transparent to the encoder/decoder which still deals with single 1 byte blocks.
Thanks to this new layer, multiple write operations on the key-value store are now aggregated.
Instead of storing one block per key, each key stores an aggregation of multiple blocks.
The guarantees offered by the erasure coding process are still kept because one aggregation only stores blocks that belong to the same stripe position.
This additional component provides a considerable speed-up comprised between $100\times$ and $1000\times$.
The way files are split in blocks, erasure coded and then aggregated to be stored on multiple Redis servers is shown in \autoref{fig:blocks}.

A \ac{lru} cache optimizes the retrieval of multiple individuals blocks.
This way, when reading a file sequentially, each aggregation of blocks is only retrieved once from the key-value store.

\subsubsection{Metadata}

When storing file contents in the storage cluster, we need to keep track where each block gets stored.
The strategy we adopted is to assign a 32 bits key to each block.
From each key, we can infer two related identifiers: a Redis key and an offset.
The Redis key points to an aggregation of blocks stored into the Redis cluster.
We then use the offset to precisely locate the block we want within the aggregation.

We store all metadata in memory.
Each file consists in a list of identifiers.
We also store the size of the file once decoded, so we can discard padding blocks.
Naturally, the space overhead of our solution is important, so our system is not able to store a large amount of data.

\subsection{Surrounding components}

Running the tester requires multiple independent services.
They need to be launched simultaneously and need to be bootstrapped to work together.
On top of these services, we want to perform measurements on the performance of different erasure codes.
The solution we adopted is the containerization of each service.
The tester along with supporting Python scripts are bundled together in a Docker image.
When an experimenter wants to start the benchmarks with, say, a Redis cluster as storage backend, he only needs to specify the desired setup in a configuration file.
The supporting Python scripts will configure the Redis cluster and then run the benchmarks and collect the results.

Thanks to the use of Docker Swarm, the setup to run the benchmarks on a local machine or on a cluster of machines is similar.
The system can use any reasonable number of Redis servers.

\subsection{Faults simulation}

The goal of the erasure coding algorithm is to provide fault tolerance.
In order to test this aspect, we introduce faults in the system. To that end, we added the possibility to specify a \textit{failure trace} when running an experiment.
A failure trace is an SQLite database in which the availability of individual nodes of a real system has been recorded.
A Redis cluster of the same size as the original system will be created.
New nodes will be instantiated and old nodes killed at the same rate as recorded in the trace.
For convenience, the user can specify a limited timeframe of the trace to use.
It is also possible to accelerate or decelerate the trace.
