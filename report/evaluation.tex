\begin{figure*}[t]
    \centering
    \input{throughput-plot.tex}
    \caption{Throughput of different erasure coding algorithms with different file sizes on a storage cluster of 100 nodes. Half confidence interval.\vs{increase space between plots, move title outside the plot, remove border around legend, make it less tall}}
    \label{fig:throughput-plot}
\end{figure*}

\section{Evaluation}
\label{sec:evaluation}

This section presents the evaluation of our \SYS prototype. 
First, we describe the evaluation settings in \autoref{sec:eval:settings}.
Then we evaluate several characteristics of the system: network throughput, encoding/decoding, request latency and scalability.
We conclude our evaluation by assessing its fault tolerance features by mean of synthetic and real-world traces.
\vs{add refs to specific sections the text is stable}.

\subsection{Evaluation Settings}
\label{sec:eval:settings}

We deploy our experiments over a cluster of machines interconnected by a \SI{1}{\giga\bit\per\second} switched network.
Each physical host features an octocore Intel Xeon CPU and \SI{8}{\giga\byte} of RAM.
We deploy \acp{vm} on top of the hosts.
The KVM hypervisor, which controls the execution of the \acp{vm}, is configured to expose the physical CPU to the guest \ac{vm} and Docker containers by mean of the \texttt{host-passthrough} option.
The \acp{vm} leverage the \texttt{virtio} module for better I/O performance.
We deploy Docker (v1.11) containers on each VM without any memory restrictions.
We use Docker Compose (v1.7.1) and Docker Swarm (v1.11.2) to orchestrate the deployment of the containers in our cluster.

\subsection{Workloads}
\label{sec:eval:workloads}
%we evaluate the various aspects or our tester by comparing two data coding algorithms against unprocessed data.
%The two algorithms under test are \acf{rs} and \acf{lrc}.
In order to evaluate the performance of \SYS, we selected the source files of two well-known open-source projects: the Apache \texttt{httpd}\footnote{\url{https://archive.apache.org/dist/httpd/httpd-2.4.18.tar.bz2}} server (v2.4.18) and GNU \texttt{bc}\footnote{\url{https://ftp.gnu.org/gnu/bc/bc-1.06.tar.gz}} (v1.06). 
We chose them because they differ in the total number of files and storage requirements, as can be seen in \autoref{fig:overhead-table}.
Moreover, we created a synthetic archive containing 1000 files, each consisting of 10 random bytes (\SI{10}{\byte} in the remainder).
We store and read files contained in these archives from the \SYS's \ac{fuse} filesystem. 
\vs{STOPPED HERE}

\begin{figure}
    \centering
    \input{checksum-plot.tex}
    \caption{Fault tolerance of: no erasure coding (Null), \acl{lrc} $\left(10,6,5\right)$ and \acl{rs} $\left(10,4\right)$. Data written on 100 nodes, and then read after killing each node.\vs{remove border around legend, reduce lenght of the horiz bar for each element}}
    \label{fig:checksum-plot}
\end{figure}

\subsection{Fault-tolerance}
\label{subsec:fault-tolerance}

One of the most important, or perhaps the most important characteristic of an erasure coding algorithm is its tolerance to faults.
A good code should ensure that data stays completely available when a small fraction of servers goes down.
We quantified the fault tolerance of \acf{rs}, \acf{lrc} and no erasure coding.
We instantiated a storage cluster of 100 nodes, and extracted the contents of an archive in it.
We used 2 different archives: \SI{10}{\byte}, which is an ideal scenario and httpd, which is our real-world scenario.
After the files were extracted, we checked the integrity of each file contained in the storage cluster.
We then killed storage nodes one after the other, and checked the files integrity before each step.

\autoref{fig:checksum-plot} shows the availability ratio of individual files with regard to the ratio of dead nodes.
It shows that \ac{rs} provides better fault-tolerance than \ac{lrc}.
It also demonstrates that the larger files contained in httpd are more prone to failures, as a single damaged stripe within a file will corrupt the entire file.

\subsection{Read/write performance and overhead of \acs{fuse}}
\label{subsec:rw-perf}

The plot in \autoref{fig:throughput-plot} shows the user-facing throughput of the system.
We write three files of different sizes and containing random data to the filesystem, and then read them.
To measure the \textit{\ac{fuse}} case, read and write operations are performed against the mountpoint exposed by the tester.
The \textit{Direct} case bypasses the \ac{fuse} layer, and directly calls Java methods.

For all cases except \textit{\ac{fuse} write}, we can see that larger file sizes yield better results.
Performance when writing increasingly bigger files using \ac{fuse} decreases because some buffers have to be resized as the file grows.
Larger writes lead to more resizing operations.

\ac{rs} is faster in writing than \ac{lrc}, notably because it requires less blocks to be written to the system,
On read operations, \ac{lrc} is inherently faster as it requires the same number of blocks as \ac{rs}.

The decrease in performance caused by \ac{fuse} ranges between \SI{-19.8}{\percent} and \SI{-41}{\percent} for read operations, and between \SI{-33.6}{\percent} and \SI{-84.9}{\percent} for write operations.

\begin{table*}
    \centering
    \caption{Various metrics measured when storing files in the system.\vs{use same names as those in the plots}}
    \input{overheads-table.tex}
    \label{fig:overhead-table}
\end{table*}

\subsection{Storage overheads}
\label{subsec:storage-overheads}

\autoref{fig:overhead-table} shows various metrics relative to the storage space used when unpacking 4 different archives into the filesytem exposed by the tester.
The \enquote{10 bytes} archive constitutes a worst case, as each file is stored as a single stripe in the system.
In that case, the number of Redis keys is strictly equal to the number of blocks in the system.
Storing a 10 bytes file into our system takes 120 bytes, before base 64 applied.
Each byte is encoded as a 32 bits integer by the erasure codes.
Additionally, each aggregation of blocks needs two 32 bits integers to store its decoded size.
This explains the overhead of $12\times$.
For real archives, the overhead asymptotically approaches $4\times$, as the two 32 bits integers are only needed once per aggregation.

The overheads cited above reason upon the binary size.
As Redis can only store strings, we have to apply base 64 encoding to the blocks, adding \SI{33}{\percent} storage overhead.

The overhead due to erasure coding is always governed by the number of parity blocks added.

\begin{figure}
    \centering
    \input{latency-plot.tex}
    \caption{Cumulative distribution of the latency observed when writing a \SI{16}{\mebi\byte} file to a storage cluster without erasure coding.\vs{use same form factor of Fig 4, remove border around legend, reduce lenght of the horiz bar for each element }}
    \label{fig:latency-plot}
\end{figure}

\subsection{Cluster size impact on latency}
\label{subsec:latency}

In order to test the impact of the cluster size on performances, we measured the time needed to write \SI{16}{\mebi\byte} to storage clusters of various sizes.
The results are shown as a \ac{cdf} in \autoref{fig:latency-plot}.
We can see that the growth of the cluster size has an impact on performance, although it is limited.

\begin{figure*}
    \centering
    \input{traffic-plot.tex}
    \caption{Network throughput between the encoder and 100 Redis storage servers when the httpd archive is written and then read. 5 nodes are brutally killed before measuring a degraded read.}
    \label{fig:traffic-plot}
\end{figure*}

\subsection{Network traffic to/from storage nodes}
\label{subsec:network-traffic}

We evaluated the network capacity used by \ac{rs} and \ac{lrc}.
We extracted the \textit{httpd} archive into the system while monitoring network traffic between the encoder and all Redis storage nodes.
We then did the reverse operation, once with all storage nodes intact, and once after killing \SI{5}{\percent} of the nodes.

The results are displayed in \autoref{fig:traffic-plot}.
They show that \ac{lrc} needs more time to complete at roughly the same speed as \ac{rs}, due to 2 more blocks per stripe.
For read operations, \ac{lrc} is equivalent to \ac{rs} when the cluster is intact.
The notable characteristic of \ac{lrc} is its increased block locality compared to \ac{rs}.
We can clearly observe that it effectively lowers the network usage as far as degraded reads are concerned.

\begin{figure}
    \centering
    \input{trace-plot.tex}
    \caption{Top plot: graphical representation of the number of nodes available at a given time as recorded in the trace file. Bottom plots: network traffic incurred by writing \textit{httpd} to the storage cluster, moving blocks between servers when scaling up, and repairing blocks when nodes die.}
    \label{fig:trace-plot}
\end{figure}

\subsection{Impact of real faults on network usage}
\label{subsec:fault-trace}

One of the unique features of our erasure codes tester is the possibility to replicate recorded failure patterns.
We used this feature to evaluate the impact on network throughput when faults happen.
We replicated a \SI{7}{\hour} subset of the fault trace recorded by the authors of \autocite{websites02}.
We initialized our cluster with 117 machines, and extracted the \textit{httpd} archive in it.
During the whole experiment, we tracked the network traffic occurring at the storage nodes.
Then, when a new node joined the system, we migrated some blocks towards it.
When an existing node departed, we scrubbed the blocks stored in the system and repaired incomplete stripes.

In \autoref{fig:trace-plot}, the uppermost plot shows the size of the storage cluster.
An increase indicates a node joining the system while a decrease marks a failure.
The two plots in the lower part of \autoref{fig:trace-plot} show the network throughput that was observed when erasure coding the data using \ac{rs} versus \ac{lrc}.
