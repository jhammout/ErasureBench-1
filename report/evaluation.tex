\section{Evaluation}
\label{sec:evaluation}

This section presents the evaluation of our \SYS prototype. 
First, we describe our evaluation settings and workload characteristics, respectively in Section~\ref{sec:eval:settings} and Section~\ref{sec:eval:workloads}.
Then we evaluate our system against several metrics: encoding/decoding throughput performance with and without the FUSE layer, network throughput, the scalability, request latency.
We conclude our evaluation by assessing its fault tolerance features by mean of synthetic and real-world traces.

\subsection{Evaluation Settings}
\label{sec:eval:settings}

We deploy our experiments over a cluster of \num{20} machines interconnected by a \SI{1}{\giga\bit\per\second} switched network.
Each host features an 8-core Intel Xeon CPU and \SI{8}{\giga\byte} of RAM.
We deploy \acp{vm} on top of the hosts.
We use KVM as a hypervisor.
To mitigate the performance losses, we expose the physical CPU to guest \acp{vm} by mean of the \texttt{host-passthrough} option.
We leverage the \texttt{virtio} module for better I/O performance.
All the components of the system are packaged as Docker images.
Specifically, we installed Docker (v1.11.2) on each \ac{vm}, and configured it without any memory restrictions.
The deployment and orchestration of the containers rely on Docker Compose (v1.7.1) and Docker Swarm (v1.2.3).

\subsection{Workloads}
\label{sec:eval:workloads}
%we evaluate the various aspects or our tester by comparing two data coding algorithms against unprocessed data.
%The two algorithms under test are \acf{rs} and \acf{lrc}.
In order to evaluate the performance of \SYS, we selected the source files of two well-known open-source projects: the Apache \texttt{httpd}\footnote{\url{https://archive.apache.org/dist/httpd/httpd-2.4.18.tar.bz2}} server (v2.4.18) and GNU \texttt{bc}\footnote{\url{https://ftp.gnu.org/gnu/bc/bc-1.06.tar.gz}} (v1.06). 
We chose them because they differ in the total number of files and storage requirements.
Moreover, we created a synthetic archive containing 1000 files, each consisting of 10 random bytes (\SI{10}{\byte} in the remainder).
We write and read files stored in these archives into/from the \SYS partition. 
\autoref{fig:overhead-table} details further the workloads.
In particular, we highlight 1) the storage overhead induced by the different erasure codes, and 2) the cost of the serialization technique used to save the data in the storage cluster.
We indicate the number of Redis keys used to index the blocks of the encoded archives and the size in \si{\mega\byte} of the binary blocks. 
Redis can only store strings, hence we apply a Base64 encoding to the blocks, incurring on average on a \SI{33}{\percent} storage overhead.

\begin{table}
    \centering
    \caption{Workload characterization and erasure-coding overhead. Sizes are given in \si{\mega\byte}.}
    \input{overheads-table.tex}
    \label{fig:overhead-table}
\end{table}

\subsection{Microbenchmark: encoding/decoding throughput}%and overhead of \acs{fuse}
\label{subsec:rw-perf}
We begin our evaluation by measuring the raw encoding throughput of our \ac{rs} and \ac{lrc} encoder implementations.
We compare them against an naive encoder (No EC) that splits and returns the same blocks, without actually encoding data.
\autoref{table:codes-performance-table} presents our results.
We observe that the encoding throughput consistently improves with the size of the blocks and achieves up to 15.3\si{\mega\byte\per\second} with \ac{lrc} and \SI{64}{\mega\byte}.
When decoding, we achieve 2.2\si{\mega\byte\per\second} and 2.9\si{\mega\byte\per\second}, respectively for \ac{rs} and \ac{lrc}.
These performances can be explained by the non-optimized nature of the implementation and the lack of native hardware support, which normally improves by orders of magnitude the performances~\cite{Burihabwa2016}. 
We leave as future work the integration of more efficient implementations.

\begin{table}
    \centering
    \caption{Encoding throughput of the different encoiders in \si{\mega\byte\per\second}}
    \input{codes-performance-table.tex}
    \label{table:codes-performance-table}
\end{table}

\subsection{Microbenchmark: read/write}
Next, we measure the read/write performances of the encoders in a larger context, that is when plugged into the \SYS system.
In this experiment, we setup a Redis cluster of 100 nodes.
We compare the performances of \SYS against a modified version of the system (\textit{Direct}) that completely bypass the \ac{fuse} layer.
For both systems, we compare the performances of \ac{lrc} and \ac{rs} as well as against a system that does not use any erasure coding (No EC).  
%We evaluate the overhead of the \ac{fuse} layer by comparing two cases: the normal access through \ac{fuse} and a special system that we call \textit{Direct} that completely bypasses it
\autoref{fig:throughput-plot} presents our results.

\begin{figure*}[t]
    \centering
    \input{throughput-plot.tex}
    \caption{Throughput of different erasure coding algorithms with different file sizes on a storage cluster of 100 nodes. Average (over 10 run) and half confidence interval.}
    \label{fig:throughput-plot}
\end{figure*}

In all considered scenarios, with the exception of \SYS \textit{write}, larger file sizes yield better results.
When using bigger files through the \SYS's \ac{fuse} layer, the throughput degrades.
We explain this by the cost of dynamically increasing buffers in the application, as \ac{fuse} will only write up to \SI{128}{\kibi\byte} at a time.
\ac{rs} allows faster write operations than \ac{lrc} (up to \vs{fill me}$\times$), as it strictly writes fewer blocks.
For read operations, \ac{lrc} is faster (up to\vs{fill me}$\times$) as it requires the same number of blocks as \ac{rs}.
The decrease in performance caused by \ac{fuse} ranges between \SI{-19.8}{\percent} and \SI{-41}{\percent} for read operations, and between \SI{-33.6}{\percent} and \SI{-84.9}{\percent} for write operations.


%\subsection{Storage overheads}
%\label{subsec:storage-overheads}
%
%\autoref{fig:overhead-table} shows various metrics relative to the storage space used when unpacking 4 different archives into the filesytem exposed by the tester.
%The \enquote{10 bytes} archive constitutes a worst case, as each file is stored as a single stripe in the system.
%In that case, the number of Redis keys is strictly equal to the number of blocks in the system.
%Storing a 10 bytes file into our system takes 120 bytes, before base 64 is applied.
%Each byte is encoded as a 32 bits integer by the erasure codes.
%Additionally, each aggregation of blocks needs two 32 bits integers to store its decoded size.
%This explains the overhead of $12\times$.
%For real archives, the overhead asymptotically approaches $4\times$, as the two 32 bits integers are only needed once per aggregation.
%The overheads cited above reason upon the binary size.
%As Redis can only store strings, we have to apply base 64 encoding to the blocks, adding \SI{33}{\percent} storage overhead.
%The overhead due to erasure coding is always governed by the number of parity blocks added.

\subsection{Scalability}
\label{subsec:latency}

To evaluate the scalability of the system, we measure the impact of the backend Redis cluster size to the observed latency of regular file-system operations.
To do so, we deploy up to 100 storage nodes.
We report the observed latency (as returned by the Linux's \texttt{time} tool) to write \SI{16}{\mebi\byte} into the \SYS partition.
\autoref{fig:latency-plot} presents our result as a Cumulative Distribution Function (CDF) of the latencies.
We observe that the choice of the erasure coding technique has a relevant impact on the latency of the operations. 
Instead, the Redis cluster can be scaled up without major penalties.
For instance, using \ac{lrc} the median (50th) latency is at 22.5s for a 20-nodes cluster, and up to 24.5s for 100-nodes cluster. 
Note that when using real-world files (such as those in the \texttt{httpd} archive presented in Section~\ref{sec:eval:workloads}) the latencies are on average 0.11s. 
We believe these results justify a large-scale deployment with hundrends of storage nodes. 

\begin{figure}[th]
    \centering
    \input{latency-plot.tex}
    \caption{Cumulative distribution of the latency observed when writing a \SI{16}{\mebi\byte} file to a storage cluster.}
    \label{fig:latency-plot}
\end{figure}

\subsection{Fault-tolerance}
\label{subsec:fault-tolerance}

One of the most important, or perhaps the most important characteristic of an erasure coding algorithm is its tolerance to faults.
A good code should ensure that data stays completely available when a small fraction of servers goes down.
We quantified the fault tolerance of \acf{rs}, \acf{lrc} and no erasure coding.
We instantiated a storage cluster of 100 nodes, and extracted the contents of an archive in it.
We used 2 different archives: \SI{10}{\byte}, which is an ideal scenario and httpd, which is our real-world scenario.
After the files were extracted, we checked the integrity of each file contained in the storage cluster.
We then killed storage nodes one after the other, and checked the files integrity before each step.

\autoref{fig:checksum-plot} shows the availability ratio of individual files with regard to the ratio of dead nodes.
It shows that \ac{rs} provides better fault-tolerance than \ac{lrc}.
It also demonstrates that the larger files contained in httpd are more prone to failures, as a single damaged stripe within a file will corrupt the entire file.

\begin{figure}[t]
    \centering
    \input{checksum-plot.tex}
    \caption{Fault tolerance for different encoders: No EC, \ac{lrc} $\left(10,6,5\right)$ and \ac{rs} $\left(10,4\right)$. Redis cluster of 100 nodes. Data checked for availability after progressively killing each Redis node.}
    \label{fig:checksum-plot}
\end{figure}


\subsection{Network traffic to/from storage nodes}
\label{subsec:network-traffic}

We evaluated the network capacity used by \ac{rs} and \ac{lrc}.
We extracted the \textit{httpd} archive into the system while monitoring network traffic between the encoder and all Redis storage nodes.
We then did the reverse operation, once with all storage nodes intact, and once after killing \SI{5}{\percent} of the nodes.

The results are displayed in \autoref{fig:traffic-plot}.
They show that \ac{lrc} needs more time to complete at roughly the same speed as \ac{rs}, due to 2 more blocks per stripe.
For read operations, \ac{lrc} is equivalent to \ac{rs} when the cluster is intact.
The notable characteristic of \ac{lrc} is its increased block locality compared to \ac{rs}.
We can clearly observe that it effectively lowers the network usage as far as degraded reads are concerned.
\begin{figure*}
    \centering
    \input{traffic-plot.tex}
    \caption{Network throughput between the encoder and 100 Redis storage servers when the httpd archive is written and then read. 5 nodes are brutally killed before measuring a degraded read.}
    \label{fig:traffic-plot}
\end{figure*}



\subsection{Impact of real faults on network usage}
\label{subsec:fault-trace}

One of the unique features of our erasure codes tester is the possibility to replicate recorded failure patterns.
We used this feature to evaluate the impact on network throughput when faults happen.
We replicated a \SI{7}{\hour} subset of the fault trace recorded by the authors of \autocite{websites02}.
We initialized our cluster with 117 machines, and extracted the \textit{httpd} archive in it.
During the whole experiment, we tracked the network traffic occurring at the storage nodes.
Then, when a new node joined the system, we migrated some blocks towards it.
When an existing node departed, we scrubbed the blocks stored in the system and repaired incomplete stripes.

In \autoref{fig:trace-plot}, the uppermost plot shows the size of the storage cluster.
An increase indicates a node joining the system while a decrease marks a failure.
The two plots in the lower part of \autoref{fig:trace-plot} show the network throughput that was observed when erasure coding the data using \ac{rs} versus \ac{lrc}.
The leftmost peak of both plots is cut so that the rest of the plot stays 
readable.
This peak consists in the writing of the archive to the filesystem.
The throughput at that moment reached \SI{3.81}{\mega\byte\per\second} using 
\ac{rs}, and \SI{3.90}{\mega\byte\per\second} using \ac{lrc}.

\begin{figure}
    \centering
    \input{trace-plot.tex}
    \caption{Top plot: graphical representation of the number of nodes available at a given time as recorded in the trace file. Bottom plots: network traffic incurred by writing \textit{httpd} to the storage cluster, moving blocks between servers when scaling up, and repairing blocks when nodes die.}
    \label{fig:trace-plot}
\end{figure}