\chapter{Related work}

\section{XORing Elephants: Novel Erasure Codes for Big Data \autocite{XorbasVLDB}}

The article presents a new family of erasure codes called \acp{lrc}.
These codes enable local repair of faulty data.
With traditional erasure codes like Reed-Solomon, the cumulative size of the blocks needed to repair a file has to be bigger or equal than the original size of the file.
With \ac{lrc}, a failure affecting a small number of blocks can be repaired using a smaller number of clean blocks.
The authors implemented their algorithm in Hadoop HDFS and deployed a test to Facebook clusters.
They measured that the repair process of \ac{lrc} uses half the disk and network bandwidth compared to Reed-Solomon, at the expense of \SI{14}{\percent} more storage overhead.

\section{A Performance Evaluation of Erasure Coding Libraries for Cloud-Based Data Stores \autocite{Burihabwa2016}}

The authors evaluate the performances of different erasure coding libraries.
In order to do the job, they developed a system that is similar to the one presented in this paper.
Their system is mainly geared towards the measurement of read/write throughput and data storage overhead of each erasure coding algorithm.
The algorithms that they tested are implemented in low-level languages.
The interface that their system exposes is a REST API exposed through HTTP.
