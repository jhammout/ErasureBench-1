\section{Related work}

\subsection{XORing Elephants: Novel Erasure Codes for Big Data}

The article \autocite{XorbasVLDB} presents a new family of erasure codes called \acp{lrc}.
These codes enable local repair of faulty data.
With traditional erasure codes like Reed-Solomon, the cumulative size of the blocks needed to repair a file has to be bigger or equal than the original size of the file.
With \ac{lrc}, a failure affecting a small number of blocks can be repaired using a smaller number of clean blocks.
The authors implemented their algorithm in Hadoop HDFS and deployed a test to Facebook clusters.
They measured that the repair process of \ac{lrc} uses half the disk and network bandwidth compared to Reed-Solomon, at the expense of \SI{14}{\percent} more storage overhead.

\subsection{A Performance Evaluation of Erasure Coding Libraries for Cloud-Based Data Stores}

The authors evaluate the performances of different erasure coding libraries \autocite{Burihabwa2016}.
In order to do the job, they developed a system that is similar to the one presented in this paper.
Their system is mainly geared towards the measurement of read/write throughput and data storage overhead of each erasure coding algorithm.
The algorithms that they tested are implemented in low-level languages.
The interface that their system exposes is a REST API exposed through HTTP.

\vs{to include \url{https://www.cs.utexas.edu/~lorenzo/papers/Silberstein14Lazy.pdf} }

\vs{to include and explain the differences: \url{https://www.usenix.org/legacy/event/fast08/wips_posters/luo-wip.pdf} cite this: 
@phdthesis{Luo:2011:HDI:2338412,
 author = {Luo, Jianqiang},
 advisor = {Xu, Lihao},
 title = {Hyfs: Design and Implementation of a Reliable File System},
 year = {2011},
 isbn = {978-1-124-86527-0},
 note = {AAI3469996},
 publisher = {Wayne State University},
 address = {Detroit, MI, USA},
}
}